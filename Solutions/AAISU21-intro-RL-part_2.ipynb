{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2554c174",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>A practial introduction to policy search in RL </h1><br>\n",
    "    <span>Algerian AI Summer University, August 2021</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa6869",
   "metadata": {},
   "source": [
    "# 2. Implementing Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e95cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:49:36.583234Z",
     "start_time": "2021-08-19T08:49:36.578241Z"
    }
   },
   "source": [
    "## Imports, configuration, and useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf538a",
   "metadata": {},
   "source": [
    "First, installing some libraries if not done already in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb462f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:39.621813Z",
     "start_time": "2021-08-24T12:01:34.738655Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.9.0 torchvision pyvirtualdisplay gym numpy pandas python-box plotly tqdm\n",
    "!sudo apt-get install xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a45b64d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:46.939134Z",
     "start_time": "2021-08-24T12:01:46.933464Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46de9868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:47.214502Z",
     "start_time": "2021-08-24T12:01:47.208560Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f414f9f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:47.455174Z",
     "start_time": "2021-08-24T12:01:47.448961Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ac5d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:53:19.421884Z",
     "start_time": "2021-08-19T08:53:19.412198Z"
    }
   },
   "source": [
    "A utility function to display short videos of the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e104be9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:47.918272Z",
     "start_time": "2021-08-24T12:01:47.908799Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import base64\n",
    "\n",
    "def show_episode(env, policy):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        logs_dir = tmpdir\n",
    "        env = Monitor(env, logs_dir, force=True, video_callable=lambda episode: True)\n",
    "        interact(env, policy)\n",
    "        html = []\n",
    "        for mp4 in Path(logs_dir).glob(\"*.mp4\"):\n",
    "            video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "            html.append('''<video alt=\"{}\" autoplay \n",
    "                          loop controls style=\"height: 400px;\">\n",
    "                          <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                     </video>'''.format(mp4, video_b64.decode('ascii')))\n",
    "        ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c83acf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:48.382329Z",
     "start_time": "2021-08-24T12:01:48.311576Z"
    }
   },
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(1200, 800))\n",
    "display.start();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7199487",
   "metadata": {},
   "source": [
    "Setting the random seeds to make the random outputs reproducible. Mandatory for debugging, developing algorithms, but proscribed when running exeperiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d83586dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:48.957713Z",
     "start_time": "2021-08-24T12:01:48.951831Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c37fd",
   "metadata": {},
   "source": [
    "`torch` imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1d1a8e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:49.848917Z",
     "start_time": "2021-08-24T12:01:49.554627Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65fdade",
   "metadata": {},
   "source": [
    "Setting the random seeds to make the random outputs reproducible. Mandatory for debugging, developing algorithms, but proscribed when running exeperiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c86ace1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:50.416740Z",
     "start_time": "2021-08-24T12:01:50.407794Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed=seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f608e",
   "metadata": {},
   "source": [
    "Plotting imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7606bcbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:51.007609Z",
     "start_time": "2021-08-24T12:01:51.001717Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac226c6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:51.372029Z",
     "start_time": "2021-08-24T12:01:51.322732Z"
    }
   },
   "outputs": [],
   "source": [
    "from box import Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce2171f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:51.681879Z",
     "start_time": "2021-08-24T12:01:51.609658Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee9a88c",
   "metadata": {},
   "source": [
    "## The env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab8850b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:52.401355Z",
     "start_time": "2021-08-24T12:01:52.393795Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fa534e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:52.895091Z",
     "start_time": "2021-08-24T12:01:52.889780Z"
    }
   },
   "outputs": [],
   "source": [
    "env.seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1431ed",
   "metadata": {},
   "source": [
    "Keep in mind this is by default a \"wrapped\" environment, to limit the number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbe646c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:54.477025Z",
     "start_time": "2021-08-24T12:01:54.450975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CartPoleEnv<CartPole-v0>>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a6fe77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:54.881329Z",
     "start_time": "2021-08-24T12:01:54.871623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._max_episode_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c453c01",
   "metadata": {},
   "source": [
    "To get the unwrapped env, simply called the `unwrapped` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aec0e41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:55.318507Z",
     "start_time": "2021-08-24T12:01:55.308747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.envs.classic_control.cartpole.CartPoleEnv at 0x7f39a67f2c40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430bbd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:47:22.683420Z",
     "start_time": "2021-08-19T08:47:22.591097Z"
    }
   },
   "source": [
    "## The policy $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054f963",
   "metadata": {},
   "source": [
    "A policy is a function of the observation (or the state) \n",
    "$$\n",
    "a = \\theta(s)\n",
    "$$\n",
    "or \n",
    "$$\n",
    "a \\sim \\theta(a|s)\n",
    "$$\n",
    "in the stochastic case. \n",
    "\n",
    "One way to learn the policy is to parametrize it with a neural network and then infer the weights of the network $\\theta$ an optimization technique (gradient-descent for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0de4d881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:56.212180Z",
     "start_time": "2021-08-24T12:01:56.209156Z"
    }
   },
   "outputs": [],
   "source": [
    "# def make_policy(input_dim, n_output)\n",
    "input_dim = env.observation_space.shape[0]\n",
    "hidden_dim = 256\n",
    "output_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81b3d1a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:57.108941Z",
     "start_time": "2021-08-24T12:01:57.095256Z"
    }
   },
   "outputs": [],
   "source": [
    "class DescretePolicy(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.base_network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = x.float()  # use .unsqueeze(dim=0) if you want to treat a batch of states\n",
    "        x = self.base_network(x)\n",
    "        dist = D.Categorical(probs=x)\n",
    "        action = dist.sample().squeeze()\n",
    "        return action, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cec18b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:57.455973Z",
     "start_time": "2021-08-24T12:01:57.429712Z"
    }
   },
   "outputs": [],
   "source": [
    "policy = DescretePolicy(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17c48479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:57.810596Z",
     "start_time": "2021-08-24T12:01:57.801022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescretePolicy(\n",
       "  (base_network): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (5): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e0c6fe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:58.192040Z",
     "start_time": "2021-08-24T12:01:58.189614Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model_parameters(model: nn.Module, path: str):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "def save_model_parameters(model: nn.Module, path: str):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994206d",
   "metadata": {},
   "source": [
    "and we need a training function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f1cab",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c047e6",
   "metadata": {},
   "source": [
    "We will need to define a couple of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37169858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:01:59.380660Z",
     "start_time": "2021-08-24T12:01:59.375642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise \n",
    "# compute the returns in one pass\n",
    "    \n",
    "def compute_return(rewards, discount):\n",
    "    discounted_return = [0]\n",
    "    for reward in reversed(rewards): \n",
    "        discounted_return.append(reward + discount * discounted_return[-1])\n",
    "\n",
    "    discounted_return = np.array(discounted_return)\n",
    "    discounted_return = discounted_return[1:][::-1]\n",
    "    return discounted_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7d82b84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:00.646073Z",
     "start_time": "2021-08-24T12:02:00.640357Z"
    }
   },
   "outputs": [],
   "source": [
    "def interact(env, policy, horizon=None):\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    entropies = []\n",
    "    t, done = 1, False\n",
    "    obs = env.reset()\n",
    "    while t != horizon and not done:\n",
    "        action, dist = policy(obs)\n",
    "        new_obs, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        entropies.append(dist.entropy())\n",
    "        \n",
    "        obs = new_obs\n",
    "        t += 1\n",
    "    env.close()\n",
    "    return rewards, log_probs, entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce5efb77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:01.558848Z",
     "start_time": "2021-08-24T12:02:01.088900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"/tmp/tmpyr27c0hw/openaigym.video.0.14872.video000000.mp4\" autoplay \n",
       "                          loop controls style=\"height: 400px;\">\n",
       "                          <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "                     </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_episode(env, policy)  # showing a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af978597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:02.966225Z",
     "start_time": "2021-08-24T12:02:02.959639Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize(env, policy, optimizer, discount, batch_size):\n",
    "\n",
    "    returned = Box()\n",
    "    returned.loss = []\n",
    "    returned.reward = []\n",
    "    returned.entropy = []\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for _ in trange(batch_size, desc='batch', leave=False, disable=False):\n",
    "        rewards, log_probs, entropies = interact(env, policy, 200)  # FIXME: hard-coded episode length\n",
    "        discounted_return = compute_return(rewards, discount)\n",
    "        undiscounted_return = np.sum(rewards)\n",
    "\n",
    "        # we normalize the returns \n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        discounted_return = (discounted_return - discounted_return.mean()) / (discounted_return.std() + eps)\n",
    "\n",
    "        loss = []\n",
    "        for lp, r in zip(log_probs, discounted_return):\n",
    "            loss.append(lp * r)\n",
    "\n",
    "        loss = torch.stack(loss)\n",
    "        entropies = torch.stack(entropies)\n",
    "        \n",
    "        if any(torch.isnan(loss)):\n",
    "            breakpoint()\n",
    "            \n",
    "        loss = loss.sum()\n",
    "        loss = -loss\n",
    "        loss.backward()\n",
    "\n",
    "        returned.loss.append(loss.item())\n",
    "        returned.reward.append(undiscounted_return)\n",
    "        returned.entropy.append(entropies.mean().item())\n",
    "\n",
    "\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    returned.loss = np.mean(returned.loss)\n",
    "    returned.reward = np.mean(returned.reward)\n",
    "    returned.entropy = np.mean(returned.entropy)\n",
    "\n",
    "    return returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c017f4ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:04.221434Z",
     "start_time": "2021-08-24T12:02:04.218615Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(env, policy, n_episodes=100):\n",
    "    ret = Box()\n",
    "    with torch.no_grad():\n",
    "        rewards = [interact(env, policy)[0] for ep in range(n_episodes)]    \n",
    "    rewards = [np.sum(r) for r in rewards]\n",
    "    ret[\"reward_mean\"] = np.mean(rewards)\n",
    "    ret[\"reward_std\"] = np.std(rewards)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e140ced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:04.662274Z",
     "start_time": "2021-08-24T12:02:04.657275Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_plot():\n",
    "    fig = make_subplots(rows=3, cols=1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[],\n",
    "            y=[],\n",
    "            line=dict(color=\"rgb(0,100,80)\"),\n",
    "            mode=\"lines\",\n",
    "            name=\"reward\",\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[],\n",
    "            y=[],\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(0,100,80,0.2)\",\n",
    "            line=dict(color=\"rgba(255,255,255,0)\"),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=[], y=[], line=dict(color=\"#D55D71\"), mode=\"lines\", name=\"entropy\"),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=[], y=[], line=dict(color=\"#9F5EDE\"), mode=\"lines\", name=\"loss\"),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.update_layout(height=800)\n",
    "    return go.FigureWidget(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c656ba7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:05.533427Z",
     "start_time": "2021-08-24T12:02:05.530214Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_plot(fig, data):\n",
    "    rew_std_upper = data.reward_mean + data.reward_std\n",
    "    rew_std_lower = data.reward_mean - data.reward_std\n",
    "\n",
    "    with fig.batch_update():\n",
    "        fig.data[0].x = data.epoch\n",
    "        fig.data[0].y = data.reward_mean\n",
    "        fig.data[1].x = pd.concat((data.epoch, data.epoch.iloc[::-1]))\n",
    "        fig.data[1].y = pd.concat((rew_std_upper, rew_std_lower.iloc[::-1]))\n",
    "        fig.data[2].x = data.epoch\n",
    "        fig.data[2].y = data.entropy\n",
    "        fig.data[3].x = data.epoch\n",
    "        fig.data[3].y = data.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51a7a0",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a07c1d",
   "metadata": {},
   "source": [
    "Some training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19f4d261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:33.114942Z",
     "start_time": "2021-08-24T12:02:33.109973Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "n_eval_ep = 10      # number of episodes used for the evaluation\n",
    "batch_size = 50\n",
    "discount = 0.99     # discount factor gamma\n",
    "lr = 3e-4           # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "702ef4b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:33.451641Z",
     "start_time": "2021-08-24T12:02:33.446054Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a3b5b4",
   "metadata": {},
   "source": [
    "Let's evaluate the policy before training (ie., a random policy) to get a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04ab7aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:34.156076Z",
     "start_time": "2021-08-24T12:02:34.007311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward (100 runs): 52.5 ± 24.965\n"
     ]
    }
   ],
   "source": [
    "print(\"Average reward (100 runs): {reward_mean} ± {reward_std:.3f}\".format(**evaluate(env, policy, n_eval_ep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca7116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:34.456889Z",
     "start_time": "2021-08-24T12:02:34.393007Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = init_plot()\n",
    "\n",
    "# this will display the figure (thanks to `InteractiveShell.ast_node_interactivity = \"all\"`)\n",
    "fig \n",
    "\n",
    "loss = []\n",
    "reward = []\n",
    "\n",
    "data = pd.DataFrame()\n",
    "current_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4ff1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T12:02:40.476495Z",
     "start_time": "2021-08-24T12:02:34.771832Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in trange(current_epoch, current_epoch + n_epochs, desc=\"epoch\", disable=False):\n",
    "\n",
    "    train_results = optimize(env, policy, optimizer, discount, batch_size)\n",
    "    eval_results = evaluate(env, policy, n_eval_ep)\n",
    "\n",
    "    info = Box()\n",
    "    info.epoch = i\n",
    "    info.update(train_results)\n",
    "    info.update(eval_results)\n",
    "\n",
    "    data = data.append(info, ignore_index=True)\n",
    "    \n",
    "    save_model_parameters(policy, \"model_cart.pt\")\n",
    "    update_plot(fig, data)\n",
    "    current_epoch += 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c23d8b62",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-23T09:54:44.411Z"
    }
   },
   "source": [
    "load_model_parameters(policy, \"model_cart.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b113ef8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T10:46:52.680715Z",
     "start_time": "2021-08-23T10:46:50.587Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average reward (100 runs): {reward_mean} ± {reward_std:.3f}\".format(**evaluate(env, policy, n_eval_ep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3a7bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T10:47:20.592414Z",
     "start_time": "2021-08-23T10:47:20.169014Z"
    }
   },
   "outputs": [],
   "source": [
    "show_episode(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb94076",
   "metadata": {},
   "source": [
    "## A continous-action env (homework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4eecc05d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T15:45:15.888022Z",
     "start_time": "2021-08-23T15:45:15.882377Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "env.seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37f06ca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T15:45:16.471979Z",
     "start_time": "2021-08-23T15:45:16.465817Z"
    }
   },
   "outputs": [],
   "source": [
    "env._max_episode_steps = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57be5a1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T15:45:16.867138Z",
     "start_time": "2021-08-23T15:45:16.862231Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = env.observation_space.shape[0]\n",
    "hidden_dim = 64\n",
    "n_hidden = 2\n",
    "output_dim = env.action_space.shape[0]  # NOTICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "875e27f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T15:45:21.482960Z",
     "start_time": "2021-08-23T15:45:21.470000Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContinuousPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, n_hidden, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.base_network = nn.Sequential(\n",
    "            *[nn.Linear(input_dim, hidden_dim), nn.ReLU()] * n_hidden,\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.layer_mean = nn.Linear(hidden_dim, output_dim)\n",
    "        self.layer_std_ = nn.Linear(hidden_dim, output_dim)\n",
    "        self.layer_std = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = x.float()  # use .unsqueeze(dim=0) if you want to treat a batch of states\n",
    "        x = self.base_network(x)\n",
    "\n",
    "        mean = self.layer_mean(x)\n",
    "        std_ = self.layer_std_(x)\n",
    "        std = self.layer_std(std_)\n",
    "        dist = D.Normal(mean, std)\n",
    "        action = dist.sample().squeeze()\n",
    "\n",
    "        return action, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6a41bcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T15:45:23.806117Z",
     "start_time": "2021-08-23T15:45:23.803176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContinuousPolicy(\n",
       "  (base_network): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (layer_mean): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (layer_std_): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (layer_std): Softplus(beta=1, threshold=20)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = ContinuousPolicy(input_dim, n_hidden, hidden_dim, output_dim)\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "09838151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T13:56:17.293897Z",
     "start_time": "2021-08-23T13:56:17.291945Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 15000\n",
    "n_eval_ep = 10      # number of episodes used for the evaluation\n",
    "batch_size = 50\n",
    "discount = 0.99     # discount factor gamma\n",
    "lr = 3e-4           # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "716fa214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T13:56:17.884375Z",
     "start_time": "2021-08-23T13:56:17.878810Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2f0039fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T13:56:18.167245Z",
     "start_time": "2021-08-23T13:56:18.163710Z"
    }
   },
   "outputs": [],
   "source": [
    "def interact(env, policy, horizon=None):\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    entropies = []\n",
    "    t, done = 1, False\n",
    "    obs = env.reset()\n",
    "    while t != horizon and not done:\n",
    "#         obs_tensor = torch.from_numpy(obs).float()\n",
    "        action, dist = policy(obs)\n",
    "        new_obs, reward, done, _ = env.step([action.item()])\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        entropies.append(dist.entropy())\n",
    "        \n",
    "        obs = new_obs\n",
    "        t += 1\n",
    "    env.close()\n",
    "    return rewards, log_probs, entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "41ffdd20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T13:56:22.691287Z",
     "start_time": "2021-08-23T13:56:22.632794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward (100 runs): -109.36355033998525 ± 20.157\n"
     ]
    }
   ],
   "source": [
    "print(\"Average reward (100 runs): {reward_mean} ± {reward_std:.3f}\".format(**evaluate(env, policy, n_eval_ep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4008eb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T13:56:23.437989Z",
     "start_time": "2021-08-23T13:56:23.359987Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = init_plot()\n",
    "\n",
    "# this will display the figure (thanks to `InteractiveShell.ast_node_interactivity = \"all\"`)\n",
    "fig \n",
    "\n",
    "loss = []\n",
    "reward = []\n",
    "\n",
    "data = pd.DataFrame()\n",
    "current_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8281a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T14:39:22.738826Z",
     "start_time": "2021-08-23T13:56:24.562287Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in trange(current_epoch, current_epoch + n_epochs, desc=\"epoch\", disable=False):\n",
    "\n",
    "    train_results = optimize(env, policy, optimizer, discount, batch_size)\n",
    "    eval_results = evaluate(env, policy, n_eval_ep)\n",
    "\n",
    "    info = Box()\n",
    "    info.epoch = i\n",
    "    info.update(train_results)\n",
    "    info.update(eval_results)\n",
    "\n",
    "    data = data.append(info, ignore_index=True)\n",
    "    \n",
    "    save_model_parameters(policy, \"model_cart.pt\")\n",
    "    update_plot(fig, data)\n",
    "    current_epoch += 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc69c863",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-23T09:54:44.411Z"
    }
   },
   "source": [
    "load_model_parameters(policy, \"model_cart.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac22cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T10:46:52.680715Z",
     "start_time": "2021-08-23T10:46:50.587Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average reward (100 runs): {reward_mean} ± {reward_std:.3f}\".format(**evaluate(env, policy, n_eval_ep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e3764",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-23T15:41:13.527Z"
    }
   },
   "source": [
    "### Does it work? Probably not. Then try to implement an actor-critic algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 EDA",
   "language": "python",
   "name": "py3.9.1_eda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
